{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48144469",
   "metadata": {},
   "source": [
    "# Smart Reply: Automated Response Suggestion for Email\n",
    "### Team Members:\n",
    "### Kare Sindhu Gonda (sindhukaregonda@my.unt.edu)\n",
    "### Pranathi Akula (PranathiAkula2@my.unt.edu)\n",
    "### Vishnubotla Chandra Sekhar (Chandrasekharvishnubotla@my.unt.edu )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bc3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55565fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c580dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       file                                            message\n",
      "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
      "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
      "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
      "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
      "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"emails.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "df = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a96be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "      <th>Body</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>arnold-j/_sent_mail/605.</td>\n",
       "      <td>Message-ID: &lt;3498983.1075857654351.JavaMail.ev...</td>\n",
       "      <td>how about benjys?</td>\n",
       "      <td>Re: ooops....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>arnold-j/_sent_mail/606.</td>\n",
       "      <td>Message-ID: &lt;20051373.1075857654373.JavaMail.e...</td>\n",
       "      <td>just a vicious rumor... my birthday's not till...</td>\n",
       "      <td>Re:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>arnold-j/_sent_mail/607.</td>\n",
       "      <td>Message-ID: &lt;16267167.1075857654394.JavaMail.e...</td>\n",
       "      <td>it was almost worth buying a ticket\\n\\n\\n\\n\\n\"...</td>\n",
       "      <td>Re: ooops....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>arnold-j/_sent_mail/608.</td>\n",
       "      <td>Message-ID: &lt;773676.1075857654425.JavaMail.eva...</td>\n",
       "      <td>---------------------- Forwarded by John Arnol...</td>\n",
       "      <td>Enron Mentions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>arnold-j/_sent_mail/609.</td>\n",
       "      <td>Message-ID: &lt;17621797.1075857654714.JavaMail.e...</td>\n",
       "      <td>---------------------- Forwarded by John Arnol...</td>\n",
       "      <td>Fw: ETKT Confirmation -</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file  \\\n",
       "3599  arnold-j/_sent_mail/605.   \n",
       "3600  arnold-j/_sent_mail/606.   \n",
       "3601  arnold-j/_sent_mail/607.   \n",
       "3602  arnold-j/_sent_mail/608.   \n",
       "3603  arnold-j/_sent_mail/609.   \n",
       "\n",
       "                                                message  \\\n",
       "3599  Message-ID: <3498983.1075857654351.JavaMail.ev...   \n",
       "3600  Message-ID: <20051373.1075857654373.JavaMail.e...   \n",
       "3601  Message-ID: <16267167.1075857654394.JavaMail.e...   \n",
       "3602  Message-ID: <773676.1075857654425.JavaMail.eva...   \n",
       "3603  Message-ID: <17621797.1075857654714.JavaMail.e...   \n",
       "\n",
       "                                                   Body  \\\n",
       "3599                                  how about benjys?   \n",
       "3600  just a vicious rumor... my birthday's not till...   \n",
       "3601  it was almost worth buying a ticket\\n\\n\\n\\n\\n\"...   \n",
       "3602  ---------------------- Forwarded by John Arnol...   \n",
       "3603  ---------------------- Forwarded by John Arnol...   \n",
       "\n",
       "                      Subject  \n",
       "3599            Re: ooops....  \n",
       "3600                      Re:  \n",
       "3601            Re: ooops....  \n",
       "3602           Enron Mentions  \n",
       "3603  Fw: ETKT Confirmation -  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracts the Email Body \n",
    "import email\n",
    "\n",
    "#parse email Body\n",
    "def parse_func(x):\n",
    "  b = email.message_from_string(x)\n",
    "  body = ''\n",
    "  if b.is_multipart():\n",
    "    for payload in b.get_payload():\n",
    "      body = body+ ' '\n",
    "      body+=payload.get_payload()\n",
    "  else:\n",
    "    body+=b.get_payload()\n",
    "\n",
    "  return body\n",
    "\n",
    "#parse email Subject\n",
    "def parse_func_subj(x):\n",
    "  b = email.message_from_string(x)\n",
    "  if b.get('Subject'):\n",
    "    subj = b.get('Subject')\n",
    "  else:\n",
    "    subj = '-'\n",
    "  return subj\n",
    "\n",
    "df['Body'] = df['message'].apply(parse_func)\n",
    "df['Subject'] = df['message'].apply(parse_func_subj)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10bd2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text and remove special characters\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df[\"Body\"] = df[\"Body\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50576a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['Body'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a1f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the body column\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(df['Body'].values)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['Body'].values)\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_len = max([len(seq) for seq in sequences])\n",
    "X = pad_sequences(sequences, maxlen=max_seq_len, padding='pre', truncating='pre')\n",
    "\n",
    "# Prepare input and target\n",
    "input_seq = X[:, :-1]\n",
    "target_seq = X[:, 1:]\n",
    "y = pad_sequences(to_categorical(X[:, 1:], num_classes=tokenizer.num_words), maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df6a9a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "181/181 [==============================] - 179s 956ms/step - loss: 0.8312 - accuracy: 0.9350 - val_loss: 0.2891 - val_accuracy: 0.9516\n",
      "Epoch 2/5\n",
      "181/181 [==============================] - 154s 851ms/step - loss: 0.2411 - accuracy: 0.9524 - val_loss: 0.2258 - val_accuracy: 0.9540\n",
      "Epoch 3/5\n",
      "181/181 [==============================] - 154s 854ms/step - loss: 0.2197 - accuracy: 0.9537 - val_loss: 0.2115 - val_accuracy: 0.9541\n",
      "Epoch 4/5\n",
      "181/181 [==============================] - 155s 857ms/step - loss: 0.2094 - accuracy: 0.9546 - val_loss: 0.2053 - val_accuracy: 0.9551\n",
      "Epoch 5/5\n",
      "181/181 [==============================] - 154s 851ms/step - loss: 0.2040 - accuracy: 0.9553 - val_loss: 0.1987 - val_accuracy: 0.9572\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=tokenizer.num_words, output_dim=32, input_shape=(None,)))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(tokenizer.num_words, activation='softmax')))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=16)\n",
    "\n",
    "# Save the tokenizer and the model\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n",
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1859d001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 4s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "#testing the Models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.json', 'r') as f:\n",
    "    tokenizer = tokenizer_from_json(f.read())\n",
    "\n",
    "# Load the model\n",
    "model = load_model('lstm_model.h5')\n",
    "\n",
    "# Load test set\n",
    "test_sequences = tokenizer.texts_to_sequences(df['Body'].values)\n",
    "# Pad sequences\n",
    "max_seq_len = 100\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(test_padded)\n",
    "\n",
    "# Print results\n",
    "for i in range(len(df)):\n",
    "    #print('Input: {}'.format(df['Body'].iloc[i]))\n",
    "    predicted_index = np.argmax(predictions[i])\n",
    "    predicted_word = tokenizer.index_word[predicted_index]\n",
    "    #print('Predicted Next Word: {}'.format(predicted_word))\n",
    "    #print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d75701df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 429ms/step\n",
      "program\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.json', 'r') as f:\n",
    "    tokenizer = tokenizer_from_json(f.read())\n",
    "\n",
    "# Load the model\n",
    "model = load_model('lstm_model.h5')\n",
    "\n",
    "\n",
    "# Input text\n",
    "input_text = 'Do you need any help?'\n",
    "\n",
    "# Preprocess the input text\n",
    "input_text = input_text.lower()\n",
    "input_text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
    "tokens = pad_sequences([tokens], maxlen=10)\n",
    "\n",
    "# Predict the next word\n",
    "probabilities = model.predict(tokens)[0]\n",
    "predicted_index = np.argmax(probabilities)\n",
    "predicted_word = tokenizer.index_word[predicted_index]\n",
    "\n",
    "# Print the predicted word\n",
    "print(predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f5653dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "program\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "input_text = 'Let me know about your status on'\n",
    "\n",
    "# Preprocess the input text\n",
    "input_text = input_text.lower()\n",
    "input_text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
    "tokens = pad_sequences([tokens], maxlen=10)\n",
    "\n",
    "# Predict the next word\n",
    "probabilities = model.predict(tokens)[0]\n",
    "predicted_index = np.argmax(probabilities)\n",
    "predicted_word = tokenizer.index_word[predicted_index]\n",
    "\n",
    "# Print the predicted word\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf053756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "floor\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "input_text = 'Please support team on'\n",
    "\n",
    "# Preprocess the input text\n",
    "input_text = input_text.lower()\n",
    "input_text = input_text.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
    "tokens = pad_sequences([tokens], maxlen=10)\n",
    "\n",
    "# Predict the next word\n",
    "probabilities = model.predict(tokens)[0]\n",
    "predicted_index = np.argmax(probabilities)\n",
    "predicted_word = tokenizer.index_word[predicted_index]\n",
    "\n",
    "# Print the predicted word\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c9fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
